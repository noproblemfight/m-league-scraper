name: Run M-League Scraper

on:
  schedule:
    # 日本時間 23:30 (UTC 14:30) に実行
    - cron: '30 14 * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 gspread google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client gspread-formatting

    - name: Run Scraper
      env:
        GOOGLE_CREDENTIALS_JSON: ${{ secrets.GOOGLE_CREDENTIALS_JSON }}
      run: |
        python mleague_scraper.py

    - name: Deploy to GitHub Pages
      if: success()
      run: |
        # Git設定
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"
        
        # 変更があるか確認
        if [ -f index.html ]; then
          # 1. 生成された index.html を退避して、元のファイルは削除する (ここが修正ポイント)
          mkdir -p /tmp/public
          cp index.html /tmp/public/
          rm index.html

          # 2. gh-pages ブランチに切り替え (なければ作る)
          git fetch origin gh-pages || true
          if git rev-parse --verify origin/gh-pages >/dev/null 2>&1; then
             git checkout gh-pages
          else
             git checkout --orphan gh-pages
             git rm -rf .
          fi

          # 3. ファイルを戻す
          cp /tmp/public/index.html .
          
          # 4. Commit & Push
          git add index.html
          git commit -m "Update dashboard: $(date)" || echo "No changes to commit"
          git push origin gh-pages
        fi
